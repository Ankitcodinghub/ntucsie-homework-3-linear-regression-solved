# ntucsie-homework-3-linear-regression-solved
**TO GET THIS SOLUTION VISIT:** [NTUCSIE Homework 3-Linear Regression Solved](https://www.ankitcodinghub.com/product/ntucsie-homework-3-solved/)


---

📩 **If you need this solution or have special requests:** **Email:** ankitcoding@gmail.com  
📱 **WhatsApp:** +1 419 877 7882  
📄 **Get a quote instantly using this form:** [Ask Homework Questions](https://www.ankitcodinghub.com/services/ask-homework-questions/)

*We deliver fast, professional, and affordable academic help.*

---

<h2>Description</h2>



<div class="kk-star-ratings kksr-auto kksr-align-center kksr-valign-top" data-payload="{&quot;align&quot;:&quot;center&quot;,&quot;id&quot;:&quot;93618&quot;,&quot;slug&quot;:&quot;default&quot;,&quot;valign&quot;:&quot;top&quot;,&quot;ignore&quot;:&quot;&quot;,&quot;reference&quot;:&quot;auto&quot;,&quot;class&quot;:&quot;&quot;,&quot;count&quot;:&quot;0&quot;,&quot;legendonly&quot;:&quot;&quot;,&quot;readonly&quot;:&quot;&quot;,&quot;score&quot;:&quot;0&quot;,&quot;starsonly&quot;:&quot;&quot;,&quot;best&quot;:&quot;5&quot;,&quot;gap&quot;:&quot;4&quot;,&quot;greet&quot;:&quot;Rate this product&quot;,&quot;legend&quot;:&quot;0\/5 - (0 votes)&quot;,&quot;size&quot;:&quot;24&quot;,&quot;title&quot;:&quot;NTUCSIE Homework 3-Linear Regression Solved&quot;,&quot;width&quot;:&quot;0&quot;,&quot;_legend&quot;:&quot;{score}\/{best} - ({count} {votes})&quot;,&quot;font_factor&quot;:&quot;1.25&quot;}">

<div class="kksr-stars">

<div class="kksr-stars-inactive">
            <div class="kksr-star" data-star="1" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="2" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="3" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="4" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="5" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>

<div class="kksr-stars-active" style="width: 0px;">
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>
</div>


<div class="kksr-legend" style="font-size: 19.2px;">
            <span class="kksr-muted">Rate this product</span>
    </div>
    </div>
<div class="page" title="Page 1">
<div class="layoutArea">
<div class="column">
Linear Regression

1. Consider a noisy target y = wTf x + ε, where x ∈ Rd+1 (including the added coordinate x0 = 1), y ∈ R, wf ∈ Rd+1 is an unknown vector, and ε is an i.i.d. noise term with zero mean and σ2 variance. Assume that we run linear regression on a training data set D = {(x1 , y1 ), . . . , (xN , yN )} generated i.i.d. from some P(x) and the noise process above, and obtain the weight vector wlin. As briefly discussed in Lecture 5, it can be shown that the expected in-sample error Ein(wlin) with respect to D is given by:

less than 0.005? Choose the correct answer; explain your answer.

[a] 25 [b] 30 [c] 35 [d] 40 [e] 45

</div>
</div>
<div class="layoutArea">
<div class="column">
ED [Ein(wlin)] = σ

For σ = 0.1 and d = 19, what is the smallest number of examples N such that ED [Ein(wlin)] is no

</div>
</div>
<div class="layoutArea">
<div class="column">
2 􏰌 d + 1􏰍 1 − N .

</div>
</div>
</div>
<div class="page" title="Page 2">
<div class="layoutArea">
<div class="column">
<ol start="2">
<li>Consider the target function f(x) = x2. Sample x uniformly from [0,1], and use all linear hy- potheses h(x) = w0 + w1 · x to approximate the target function with respect to the squared error. What are the weights (w0∗, w1∗) of the optimal hypothesis? Choose the correct answer; explain your answer.[a] (0,1) [b] (21,12)[c] (−16,1) [d] (−14,14)
[e] (13,0)

(Hint: The optimal hypothesis g∗ must reach the minimum Eout(g∗).)
</li>
<li>Following the previous problem, assume that we sample two examples x1 and x2 uniformly from [0,1] to form the training set D = {(x1,f(x1)),(x2,f(x2)}, and use linear regression to get g for approximating the target function with respect to the squared error. You can neglect the degenerate cases where x1 and x2 are the same. What is ED (|Ein(g) − Eout(g)|)? Choose the correct answer; explain your answer.[a] 1 60[b] 4 15
[c] 3 20

[d] 1 25

[e] 1 30
</li>
</ol>
Cross-Entropy Error

4. In class, we introduced our version of the cross-entropy error function

1 􏰆N

− ln θ(ynwT xn).

of the following error function is equivalent to Ein above? Choose the correct answer; explain your

</div>
</div>
<div class="layoutArea">
<div class="column">
Ein(w) = N

based on the definition of yn ∈ {−1, +1}. If we transform yn to yn′ ∈ {0, 1} by yn′ = yn +1 , which

</div>
</div>
<div class="layoutArea">
<div class="column">
answer.

[a] 1 􏰅N 􏰃y′ lnθ(wTx )+(1−y′ )ln(θ(−wTx ))􏰄

</div>
</div>
<div class="layoutArea">
<div class="column">
Nn=1n n n n [b] 1 􏰅N 􏰃y′ lnθ(−wTx )+(1−y′ )ln(θ(wTx ))􏰄

</div>
</div>
<div class="layoutArea">
<div class="column">
Nn=1n n n n

[c] 1 􏰅N 􏰃y′ lnθ(wTx )−(1−y′ )ln(θ(−wTx ))􏰄

</div>
</div>
<div class="layoutArea">
<div class="column">
Nn=1n n n n [d] 1 􏰅N 􏰃y′ lnθ(−wTx )−(1−y′ )ln(θ(wTx ))􏰄

</div>
</div>
<div class="layoutArea">
<div class="column">
Nn=1n n n n [e] none of the other choices

</div>
</div>
<div class="layoutArea">
<div class="column">
n=1

</div>
</div>
<div class="layoutArea">
<div class="column"></div>
</div>
</div>
<div class="page" title="Page 3">
<div class="layoutArea">
<div class="column">
&nbsp;

</div>
</div>
<div class="layoutArea">
<div class="column">
5. Consider a coin with an unknown head probability μ. Independently flip this coin N times to get y1, y2, . . . , yN , where yn = 1 if the n-th flipping results in head, and 0 otherwise. Define ν = N1 􏰅Nn=1 yn. How many of the following statements about ν are true? Choose the correct answer; explain your answer by briefly illustrating why those statements are true.

<ul>
<li>With probability more than 1 − δ,􏰏12 μ≤ν+ 2Nlnδfor all N ∈ N and 0 &lt; δ &lt; 1.</li>
<li>ν maximizes likelihood(μˆ) over all μˆ ∈ [0, 1].</li>
<li>ν minimizes the squared error1 􏰆N Esqr(yˆ)= N (yˆ−yn)2n=1
over all yˆ ∈ R.
</li>
<li>When 0 &lt; ν &lt; 1, it minimizes the cross-entropy error (which is similar to the cross-entropyerror for logistic regression)

Ece(yˆ)= N1 􏰆N 􏰃ynlnyˆ+(1−yn)ln(1−yˆ)􏰄n=1

over all yˆ ∈ (0, 1).

(Note: μ is similar to the role of the “target function” and μˆ is similar to the role of the “hypothesis”

in our machine learning framework.)

[a] 0 [b] 1 [c] 2 [d] 3 [e] 4

Stochastic Gradient Descent
</li>
</ul>
6. In the perceptron learning algorithm, we find one example (xn(t),yn(t)) that the current weight

vector wt mis-classifies, and then update wt by

wt+1 ← wt + yn(t)xn(t).

The algorithm can be viewed as optimizing some Ein(w) that is composed of one of the following point-wise error functions with stochastic gradient descent (neglecting any non-differentiable points of the error function). What is the error function? Choose the correct answer; explain your answer.

[a] err(w,x,y)=max(0,−ywTx) [b] err(w,x,y)=−max(0,−ywTx)

[c] err(w,x,y)=max(ywTx,−ywTx) [d] err(w,x,y)=−max(ywTx,−ywTx)

[e] none of the other choices

Multinomial Logistic Regression

</div>
</div>
</div>
<div class="page" title="Page 4">
<div class="layoutArea">
<div class="column">
7. In Lecture 6, we solve multiclass classification by OVA or OVO decompositions. One alternative to deal with multiclass classification is to extend the original logistic regression model to Multinomial Logistic Regression (MLR). For a K-class classification problem, we will denote the output space Y = {1, 2, · · · , K}. The hypotheses considered by MLR can be indexed by a matrix

||···|···| W=w1 w2 ··· wk ··· wK ,

||···|···|

that contains weight vectors (w1, · · · , wK ), each of length d+1. The matrix represents a hypothesis

h (x) = exp(wyT x)

y 􏰅Ki=1 exp(wiT x)

that can be used to approximate the target distribution P (y|x) for any (x, y). MLR then seeks for the maximum likelihood solution over all such hypotheses. For a given data set {(x1, y1), . . . , (xN , yN )} generated i.i.d. from some P(x) and target distribution P(y|x), the likelihood of hy(x) is propor- tional to 􏰎Nn=1 hyn (xn). That is, minimizing the negative log likelihood is equivalent to minimizing an Ein(W) that is composed of the following error function

K

􏰆

</div>
</div>
<div class="layoutArea">
<div class="column">
(d+1)×K

</div>
</div>
<div class="layoutArea">
<div class="column">
err(W,x,y) = −lnhy(x) = −

</div>
<div class="column">
􏰉y = k􏰊lnhk(x).

</div>
</div>
<div class="layoutArea">
<div class="column">
k=1

When minimizing Ein(W) with SGD, we update the W(t) at the t-th iteration to W(t+1) by

W(t+1) ←W(t) +η·V,

where V is a (d + 1) × K matrix whose k-th column is an update direction for the k-th weight vector. Assume that an example (xn,yn) is used for the SGD update above. What is the yn-th column of V? Choose the correct answer; explain your answer.

<ol>
<li>[a] &nbsp;(1 − hyn (xn ))xn</li>
<li>[b] &nbsp;(hyn (xn ) − 1)xn</li>
<li>[c] &nbsp;(−hyn (xn ))xn</li>
<li>[d] &nbsp;(hyn (xn ))xn</li>
<li>[e] &nbsp;none of the other choices</li>
</ol>
Nonlinear Transformation 8. Given the following training data set:

x1 = (0,1),y1 = −1 x2 = (0,−1),y2 = −1 x3 = (−1,0),y3 = +1 x4 = (1,0),y4 = +1

Use the quadratic transform Φ2(x) = (1, x1, x2, x21, x1x2, x2) and take sign(0) = 1. Which of the following weight vector w ̃ represents a linear classifier in the Z-space that can separate all the transformed examples perfectly? Choose the correct answer; explain your answer.

[a] (0,−1,0,0,0,0) [b] (0,0,−1,0,0,0) [c] (0,0,0,−1,0,0) [d] (0,0,0,0,−1,0) [e] (0,0,0,0,0,−1)

</div>
</div>
</div>
<div class="page" title="Page 5">
<div class="layoutArea">
<div class="column">
<ol start="9">
<li>Consider a feature transform Φ(x) = Γx where Γ is a (d + 1) by (d + 1) invertible matrix. For a training data set {(xn,yn)}Nn=1, run linear regression on the original data set, and get wlin. Then, run linear regression on the Φ-transformed data, and get w ̃ . For simplicity, assume that the matrix X (with every row being xTn ) satisfies that XT X is invertible. What is the relationship between wlin and w ̃ ? Choose the correct answer; explain your answer.[a] wlin = Γw ̃ [b] wlin = ΓT w ̃[c] wlin = (Γ−1)T w ̃ [d] wlin = Γ−1w ̃
[e] none of the other choices
</li>
<li>After “visualizing” the data and noticing that all x1,, x2, …, xn are distinct, Dr. Trans magicallydecides the following transform

Φ(x) = (􏰉x = x1􏰊,􏰉x = x2􏰊,…,􏰉x = xN􏰊).That is, Φ(x) is a N-dimentional vector whose n-th component is 1 if and only if x = xn. If we run linear regression after applying this transform, what is the optimal w ̃ ? Choose the correct answer; explain your answer.

[a] 1, the vector of all 1s. [b] 0, the vector of all 0s.

[c] y [d] −y

[e] none of the other choices

(Note: Be sure to also check what Ein(w ̃) is!)
</li>
</ol>
</div>
</div>
<div class="layoutArea">
<div class="column">
11. Assume that we coulple linear regression with one-versus-all decomposition for multi-class classifi- cation, and get K weight vectors w∗ . Assume that the squared error Esqr(w∗ ) for the k-th binary

</div>
</div>
<div class="layoutArea">
<div class="column">
[k] in [k]

classification problem is ek. What is the tightest upper bound of E0/1(g), where g is the multi-

</div>
</div>
<div class="layoutArea">
<div class="column">
in

class classifier formed by the one-versus-all decomposition? Choose the correct answer; explain your answer.

[a] 2􏰅Kk=1ek [b] 􏰅Kk=1 ek

[c] 21 􏰅Kk=1 ek [d] K1 􏰅Kk=1ek

[e] 1 􏰅K ek 2K k=1

</div>
</div>
</div>
<div class="page" title="Page 6">
<div class="layoutArea">
<div class="column">
Experiments with Linear and Nonlinear Models

Next, we will play with transform + linear regression for binary classification. Please use the following set for training:

<pre>         https://www.csie.ntu.edu.tw/~htlin/course/ml21fall/hw3/hw3_train.dat
</pre>
and the following set for testing (estimating Eout): https://www.csie.ntu.edu.tw/~htlin/course/ml21fall/hw3/hw3_test.dat

Each line of the data set contains one (xn,yn) with xn ∈ R10. The first 10 numbers of the line contains the components of xn orderly, the last number is yn, which belongs to {−1, +1} ⊆ R. That is, we can use those yn for either binary classification or regression.

12. (*) Consider the following homogeneous order-Q polynomial transform

Φ(x) = (1, x1, x2, …, x10, x21, x2, …, x210, …, xQ1 , xQ2 , …, xQ10).

</div>
</div>
<div class="layoutArea">
<div class="column">
Transform the training and testing data according to Φ(x) with Q = 2, and implement the linear regression algorithm on the transformed data. What is 􏰋􏰋􏰋E0/1(g) − E0/1(g)􏰋􏰋􏰋, where g is the hypoth-

esis returned by the transform + linear regression procedure? Choose the closest answer; provide your code.

[a] 0.28 [b] 0.32 [c] 0.36 [d] 0.40 [e] 0.44

<ol start="13">
<li>(*) Repeat the previous problem, but with Q = 8 instead. What is 􏰋􏰋􏰋E0/1(g) − E0/1(g)􏰋􏰋􏰋, where in outg is the hypothesis returned by the transform + linear regression procedure? Choose the closest answer; provide your code.[a] 0.30 [b] 0.35 [c] 0.40 [d] 0.45 [e] 0.50</li>
<li>(*) Repeat the previous problem, but with Φ2 (the full order-2 polynomial transform introduced in the lecture, which is of 1 + 10 + 45 + 10 dimensions) instead. What is 􏰋􏰋􏰋E0/1(g) − E0/1(g)􏰋􏰋􏰋, whereg is the hypothesis returned by the transform + linear regression procedure? Choose the closest answer; provide your code.[a] 0.33 [b] 0.41 [c] 0.49 [d] 0.57 [e] 0.65</li>
</ol>
</div>
</div>
<div class="layoutArea">
<div class="column">
6 of 7

</div>
</div>
<div class="layoutArea">
<div class="column">
in out

</div>
</div>
<div class="layoutArea">
<div class="column">
in out

</div>
</div>
</div>
<div class="page" title="Page 7">
<div class="layoutArea">
<div class="column">
15. (*) Instead of transforming to a higher dimensional space, we can also transform to a lower dimen- sional space. Consider the following 10 transforms:

</div>
</div>
<div class="layoutArea">
<div class="column">
Φ(1)(x) Φ(2)(x)

</div>
<div class="column">
= (x0, x1)

= (x0, x1, x2) …

</div>
</div>
<div class="layoutArea">
<div class="column">
Φ(10)(x)

Run Φ(i) + linear regression to get a hypothesis g . What is the minimum 􏰋􏰋􏰋E0/1(g ) − E0/1(g )􏰋􏰋􏰋

</div>
</div>
<div class="layoutArea">
<div class="column">
= (x0, x1, x2, . . . , x10)

i in i out i

</div>
</div>
<div class="layoutArea">
<div class="column">
over i? Choose the closest answer; provide your code.

[a] 1 [b] 2 [c] 3 [d] 5 [e] 8

16. (*) Consider a transform that randomly chooses 5 out of 10 dimensions. That is, Φ(x) = (x0, xi1 , xi2 , xi3 , xi4 , xi5 ), where i1 to i5 are distinct random integers uniformly and independently generated within {1, 2, . . . , 10}. Run Φ + linear regression to get a hypothesis g. What is the

average 􏰋􏰋􏰋E0/1(g ) − E0/1(g )􏰋􏰋􏰋 over 200 experiments, each generating Φ with a different random in i out i

seed? Choose the closest answer; provide your code.

[a] 0.06 [b] 0.11 [c] 0.16 [d] 0.21 [e] 0.26

</div>
</div>
<div class="layoutArea">
<div class="column">
7 of 7

</div>
</div>
</div>
